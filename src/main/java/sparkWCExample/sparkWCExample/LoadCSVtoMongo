package sparkWCExample.sparkWCExample;


import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import com.mongodb.spark.MongoSpark;
import com.mongodb.spark.config.WriteConfig;

import org.bson.Document;

public class LoadCSVtoMongo {

	public static void main(String[] args) {

		long totalTime = System.currentTimeMillis();
        
		SparkSession spark = SparkSession.builder()
			      .master("local")
			      .appName("MongoSparkConnectorIntro")
			      .config("spark.mongodb.input.uri", "mongodb://01HW324895:27017/test.myCollection")
			      .config("spark.mongodb.output.uri", "mongodb://01HW324895:27017/test.myCollection")
			      .getOrCreate();		        
		
		
		JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());
		Dataset<Row> usersDf = spark.read().csv("C:/sparkWCExample/src/main/resources/mock.csv");
			
		// Create a custom WriteConfig
	    Map<String, String> writeOverrides = new HashMap<String, String>();
	    writeOverrides.put("collection", "mockcsv2");
	    writeOverrides.put("writeConcern.w", "majority");
	    WriteConfig writeConfig = WriteConfig.create(sc).withOptions(writeOverrides);
	    System.out.println("Inside WriteToMongoDB: " + writeConfig.databaseName());	      
	       
	    MongoSpark.write(usersDf).option("collection", "hundredClub").mode("overwrite").save();
		
	    System.out.println(">>> TOTAL time ms:" + (System.currentTimeMillis() - totalTime));	     
	 
        
}
}
